{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/data_2/patrick/conda/envs/tgi-env/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import text_generation as tg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set environment variable\n",
    "import os\n",
    "os.environ['TGI_CENTRAL_ADDRESS'] = 'localhost:8765'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': '/mnt/data_2/patrick/croissantllm-models/small4_equals/', 'address': 'frightened-frank-flowers-fin-01:3000', 'owner': 'patrick', 'is_quantized': False}]\n"
     ]
    }
   ],
   "source": [
    "servers = tg.Client.list_from_central()\n",
    "print(servers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "server_addr = servers[0]['address']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = tg.Client(f\"http://{server_addr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "among the best in the country in their field of study. They are also among the best in the\n"
     ]
    }
   ],
   "source": [
    "print(client.generate(\"CMU's PhD students are\", max_new_tokens=20).generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " among the best in the country in their field of study. They are also among the best in the\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\n",
    "for response in client.generate_stream(\"CMU's PhD students are\", max_new_tokens=20):\n",
    "    if not response.token.special:\n",
    "        text += response.token.text\n",
    "print(text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Top K tokens at each step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "among the best in\n",
      "[Token(id=5684, text='among', logprob=-2.5429688, special=False), Token(id=4645, text='working', logprob=-3.4179688, special=False), Token(id=1135, text='the', logprob=-3.6757812, special=False)]\n",
      "[Token(id=1135, text='the', logprob=-0.40478516, special=False), Token(id=3108, text='those', logprob=-2.7402344, special=False), Token(id=488, text='', logprob=-2.8417969, special=False)]\n",
      "[Token(id=3284, text='best', logprob=-1.5273438, special=False), Token(id=2481, text='most', logprob=-1.5664062, special=False), Token(id=3263, text='top', logprob=-2.2148438, special=False)]\n",
      "[Token(id=1147, text='in', logprob=-0.45898438, special=False), Token(id=1171, text='and', logprob=-3.7089844, special=False), Token(id=5208, text='students', logprob=-3.9511719, special=False)]\n"
     ]
    }
   ],
   "source": [
    "resp = client.generate(\"CMU's PhD students are\", max_new_tokens=4, top_tokens=3)\n",
    "print(resp.generated_text)\n",
    "for top_tokens in resp.details.top_tokens:\n",
    "    print(top_tokens)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create 4 random sentences\n",
    "SAMPLES = [\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"The five boxing wizards jump quickly.\",\n",
    "    \"All questions asked by five watch experts amazed the judge.\",\n",
    "    \"Jack quietly moved up front and seized the big ball of wax.\",\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sync Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The quick brown fox jumps over the lazy dog.\n",
      "The quick brown fox j\n",
      "\n",
      "The first step in the process is to create a list of potential candidates. This list should include\n",
      "\n",
      "The first time I heard the term “fake news” was in the context of the \n",
      "He was a master of disguise, and he had a knack for getting into places he\n",
      "CPU times: user 36.8 ms, sys: 3.42 ms, total: 40.2 ms\n",
      "Wall time: 1.95 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for sample in SAMPLES:\n",
    "    print(client.generate(sample, max_new_tokens=20).generated_text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Async Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "async_client = tg.AsyncClient(f\"http://{server_addr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The quick brown fox jumps over the lazy dog.\n",
      "The quick brown fox j\n",
      "\n",
      "The first step in the process is to create a list of potential candidates. This list should include\n",
      "\n",
      "The first time I heard the term “fake news” was in the context of the \n",
      "He was a master of disguise, and he had a knack for getting into places he\n",
      "CPU times: user 105 ms, sys: 5.03 ms, total: 110 ms\n",
      "Wall time: 620 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "async def batch_generate():\n",
    "    return await asyncio.gather(*[async_client.generate(sample, max_new_tokens=20) for sample in SAMPLES])\n",
    "\n",
    "results = asyncio.run(batch_generate())\n",
    "for r in results:\n",
    "    print(r.generated_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tgi-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
